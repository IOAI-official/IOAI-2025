{"cells":[{"cell_type":"markdown","id":"97ae6685","metadata":{},"source":"# configs"},{"cell_type":"code","execution_count":null,"id":"4243ca5d","metadata":{"jupyter":{"source_hidden":false}},"source":"radar_configs = {\n    'c': 3e8, \n    'startFreq': 77e9, \n    'Tr': 60e-6,                    # Sweeping frequency time\n    'Idle_time': 100e-6,            # free time\n    'Fs': 10e6,                     # Sampling frequency\n    'Slope': 29.982e12,             # chirp slope\n    'Bandwidth': 60e-6 * 29.982e12, # Transmission signal bandwidth\n    'BandwidthValid': 0.767539200e9,# Effective bandwidth of the transmitted signal\n    'range_size': 256,              # range size\n    'azimuth_size': 181,            # azimuth size\n    'elevation_size': 181,          # elevation size\n    'crop_num': 3,                  # crop some indices in range domain\n    'n_chirps': 128,                # number of chirps in one frame\n    'min_azimuth': -90,             # min radar azimuth\n    'max_azimuth': 90,              # max radar azimuth\n    'min_elevation': -90,           # min radar elevation\n    'max_elevation': 90,            # max radar elevation    \n    'min_range': 1.0,               # min radar range\n    'max_range': 25.0,              # max radar range\n    'range_res': 3e8/(2*0.767539200e9), \n    'angle_res': 1\n}\n\ndimssnet_configs = {\n    'n_epoch': 10,\n    'batch_size': 2,\n    'learning_rate': 1e-5,\n    'range_size': 50,\n    'azimuth_size': 181,\n    'elevation_size': 181,\n    'doppler_size': 181, \n    'min_azimuth': -90,             # min radar azimuth\n    'max_azimuth': 90,              # max radar azimuth\n    'min_elevation': -90,           # min radar elevation\n    'max_elevation': 90,            # max radar elevation    \n    'min_range': 1.0,               # min radar range\n    'max_range': 25.0,              # max radar range\n}\n\nn_class = 5\nclass_table = {\n    -1: 'background', \n    0: 'suicase', \n    1: 'chair', \n    2: 'person',\n    3: 'wall'\n}"},{"cell_type":"markdown","id":"c39a54f5","metadata":{},"source":"# Models"},{"cell_type":"code","execution_count":null,"id":"10817ce8","metadata":{"jupyter":{"source_hidden":false}},"source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SRA_Encode(nn.Module):\n    def __init__(self):\n        super(SRA_Encode, self).__init__()\n        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv1b = nn.Conv2d(in_channels=64, out_channels=64, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2a = nn.Conv2d(in_channels=64, out_channels=128, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3a = nn.Conv2d(in_channels=128, out_channels=256, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv3b = nn.Conv2d(in_channels=256, out_channels=256, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n        self.bn1a = nn.BatchNorm2d(num_features=64)\n        self.bn1b = nn.BatchNorm2d(num_features=64)\n        self.bn2a = nn.BatchNorm2d(num_features=128)\n        self.bn2b = nn.BatchNorm2d(num_features=128)\n        self.bn3a = nn.BatchNorm2d(num_features=256)\n        self.bn3b = nn.BatchNorm2d(num_features=256)\n\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1a(self.conv1a(x)))\n        x = self.relu(self.bn1b(self.conv1b(x)))\n        x = self.relu(self.bn2a(self.conv2a(x)))\n        x = self.relu(self.bn2b(self.conv2b(x)))\n        x = self.relu(self.bn3a(self.conv3a(x)))\n        x = self.relu(self.bn3b(self.conv3b(x)))\n        \n        return x\n\nclass SRA_Decode(nn.Module):\n    def __init__(self):\n        super(SRA_Decode, self).__init__()\n        self.convt1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt3 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.prelu = nn.PReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.upsample = nn.Upsample(size=(dimssnet_configs['range_size'],\n                                          dimssnet_configs['azimuth_size']), mode='nearest')\n\n    def forward(self, x):\n        x = self.prelu(self.convt1(x))\n        x = self.prelu(self.convt2(x))\n        x = self.prelu(self.convt3(x))\n\n        return x\n\nclass DRA_Encode(nn.Module):\n    def __init__(self):\n        super(DRA_Encode, self).__init__()\n        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # (1 50 181)->(64 50 181)\n        self.conv1b = nn.Conv2d(in_channels=64, out_channels=64, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # (64 50 181)->(64 25 91)\n        self.conv2a = nn.Conv2d(in_channels=64, out_channels=128, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # (64 25 91)->(128 25 91)\n        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # (128 25 91)->(128 13 46)\n        self.conv3a = nn.Conv2d(in_channels=128, out_channels=256, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) # (128 13 46)->(256 13 46)\n        self.conv3b = nn.Conv2d(in_channels=256, out_channels=256, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # (256 13 46)->(256 7 23)\n\n        self.bn1a = nn.BatchNorm2d(num_features=64)\n        self.bn1b = nn.BatchNorm2d(num_features=64)\n        self.bn2a = nn.BatchNorm2d(num_features=128)\n        self.bn2b = nn.BatchNorm2d(num_features=128)\n        self.bn3a = nn.BatchNorm2d(num_features=256)\n        self.bn3b = nn.BatchNorm2d(num_features=256)\n\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1a(self.conv1a(x)))\n        x = self.relu(self.bn1b(self.conv1b(x)))\n        x = self.relu(self.bn2a(self.conv2a(x)))\n        x = self.relu(self.bn2b(self.conv2b(x)))\n        x = self.relu(self.bn3a(self.conv3a(x)))\n        x = self.relu(self.bn3b(self.conv3b(x)))\n\n        return x\n\nclass DRA_Decode(nn.Module):\n    def __init__(self):\n        super(DRA_Decode, self).__init__()\n        self.convt1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # (256 7 23)->(128 13 45)\n        self.convt2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # (128 13 45)->(64 25 89)\n        self.convt3 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) # (64 25 89)->(32 49 177)\n        self.prelu = nn.PReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.upsample = nn.Upsample(size=(dimssnet_configs['range_size'],\n                                          dimssnet_configs['elevation_size']), mode='nearest')\n\n    def forward(self, x):\n        x = self.prelu(self.convt1(x))\n        x = self.prelu(self.convt2(x))\n        x = self.prelu(self.convt3(x))\n\n        return x\n\nclass SRE_Encode(nn.Module):\n    def __init__(self):\n        super(SRE_Encode, self).__init__()\n        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv1b = nn.Conv2d(in_channels=64, out_channels=64, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2a = nn.Conv2d(in_channels=64, out_channels=128, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3a = nn.Conv2d(in_channels=128, out_channels=256, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv3b = nn.Conv2d(in_channels=256, out_channels=256, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n        self.bn1a = nn.BatchNorm2d(num_features=64)\n        self.bn1b = nn.BatchNorm2d(num_features=64)\n        self.bn2a = nn.BatchNorm2d(num_features=128)\n        self.bn2b = nn.BatchNorm2d(num_features=128)\n        self.bn3a = nn.BatchNorm2d(num_features=256)\n        self.bn3b = nn.BatchNorm2d(num_features=256)\n\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1a(self.conv1a(x)))\n        x = self.relu(self.bn1b(self.conv1b(x)))\n        x = self.relu(self.bn2a(self.conv2a(x)))\n        x = self.relu(self.bn2b(self.conv2b(x)))\n        x = self.relu(self.bn3a(self.conv3a(x)))\n        x = self.relu(self.bn3b(self.conv3b(x)))\n\n        return x\n\nclass SRE_Decode(nn.Module):\n    def __init__(self):\n        super(SRE_Decode, self).__init__()\n        self.convt1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt3 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.prelu = nn.PReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.upsample = nn.Upsample(size=(dimssnet_configs['range_size'],\n                                          dimssnet_configs['elevation_size']), mode='nearest')\n\n    def forward(self, x):\n        x = self.prelu(self.convt1(x))\n        x = self.prelu(self.convt2(x))\n        x = self.prelu(self.convt3(x))\n\n        return x\n    \nclass DRE_Encode(nn.Module):\n    def __init__(self):\n        super(DRE_Encode, self).__init__()\n        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv1b = nn.Conv2d(in_channels=64, out_channels=64, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2a = nn.Conv2d(in_channels=64, out_channels=128, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3a = nn.Conv2d(in_channels=128, out_channels=256, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv3b = nn.Conv2d(in_channels=256, out_channels=256, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n        self.bn1a = nn.BatchNorm2d(num_features=64)\n        self.bn1b = nn.BatchNorm2d(num_features=64)\n        self.bn2a = nn.BatchNorm2d(num_features=128)\n        self.bn2b = nn.BatchNorm2d(num_features=128)\n        self.bn3a = nn.BatchNorm2d(num_features=256)\n        self.bn3b = nn.BatchNorm2d(num_features=256)\n\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1a(self.conv1a(x)))\n        x = self.relu(self.bn1b(self.conv1b(x)))\n        x = self.relu(self.bn2a(self.conv2a(x)))\n        x = self.relu(self.bn2b(self.conv2b(x)))\n        x = self.relu(self.bn3a(self.conv3a(x)))\n        x = self.relu(self.bn3b(self.conv3b(x)))\n\n        return x\n\nclass DRE_Decode(nn.Module):\n    def __init__(self):\n        super(DRE_Decode, self).__init__()\n        self.convt1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt3 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.prelu = nn.PReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.upsample = nn.Upsample(size=(dimssnet_configs['range_size'],\n                                          dimssnet_configs['elevation_size']), mode='nearest')\n\n    def forward(self, x):\n        x = self.prelu(self.convt1(x))\n        x = self.prelu(self.convt2(x))\n        x = self.prelu(self.convt3(x))\n\n        return x\n    \nclass SRD_Encode(nn.Module):\n    def __init__(self):\n        super(SRD_Encode, self).__init__()\n        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv1b = nn.Conv2d(in_channels=64, out_channels=64, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2a = nn.Conv2d(in_channels=64, out_channels=128, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3a = nn.Conv2d(in_channels=128, out_channels=256, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv3b = nn.Conv2d(in_channels=256, out_channels=256, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n        self.bn1a = nn.BatchNorm2d(num_features=64)\n        self.bn1b = nn.BatchNorm2d(num_features=64)\n        self.bn2a = nn.BatchNorm2d(num_features=128)\n        self.bn2b = nn.BatchNorm2d(num_features=128)\n        self.bn3a = nn.BatchNorm2d(num_features=256)\n        self.bn3b = nn.BatchNorm2d(num_features=256)\n\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1a(self.conv1a(x)))\n        x = self.relu(self.bn1b(self.conv1b(x)))\n        x = self.relu(self.bn2a(self.conv2a(x)))\n        x = self.relu(self.bn2b(self.conv2b(x)))\n        x = self.relu(self.bn3a(self.conv3a(x)))\n        x = self.relu(self.bn3b(self.conv3b(x)))\n\n        return x\n\nclass SRD_Decode(nn.Module):\n    def __init__(self):\n        super(SRD_Decode, self).__init__()\n        self.convt1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt3 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.prelu = nn.PReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.upsample = nn.Upsample(size=(dimssnet_configs['range_size'],\n                                          dimssnet_configs['doppler_size']), mode='nearest')\n\n    def forward(self, x):\n        x = self.prelu(self.convt1(x))\n        x = self.prelu(self.convt2(x))\n        x = self.prelu(self.convt3(x))\n\n        return x\n    \nclass DRD_Encode(nn.Module):\n    def __init__(self):\n        super(DRD_Encode, self).__init__()\n        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv1b = nn.Conv2d(in_channels=64, out_channels=64, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv2a = nn.Conv2d(in_channels=64, out_channels=128, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv2b = nn.Conv2d(in_channels=128, out_channels=128, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.conv3a = nn.Conv2d(in_channels=128, out_channels=256, \n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.conv3b = nn.Conv2d(in_channels=256, out_channels=256, \n                                kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n\n        self.bn1a = nn.BatchNorm2d(num_features=64)\n        self.bn1b = nn.BatchNorm2d(num_features=64)\n        self.bn2a = nn.BatchNorm2d(num_features=128)\n        self.bn2b = nn.BatchNorm2d(num_features=128)\n        self.bn3a = nn.BatchNorm2d(num_features=256)\n        self.bn3b = nn.BatchNorm2d(num_features=256)\n\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.bn1a(self.conv1a(x)))\n        x = self.relu(self.bn1b(self.conv1b(x)))\n        x = self.relu(self.bn2a(self.conv2a(x)))\n        x = self.relu(self.bn2b(self.conv2b(x)))\n        x = self.relu(self.bn3a(self.conv3a(x)))\n        x = self.relu(self.bn3b(self.conv3b(x)))\n\n        return x\n\nclass DRD_Decode(nn.Module):\n    def __init__(self):\n        super(DRD_Decode, self).__init__()\n        self.convt1 = nn.ConvTranspose2d(in_channels=256, out_channels=128,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt2 = nn.ConvTranspose2d(in_channels=128, out_channels=64,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.convt3 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n                                         kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n        self.prelu = nn.PReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.upsample = nn.Upsample(size=(dimssnet_configs['range_size'],\n                                          dimssnet_configs['doppler_size']), mode='nearest')\n\n    def forward(self, x):\n        x = self.prelu(self.convt1(x))\n        x = self.prelu(self.convt2(x))\n        x = self.prelu(self.convt3(x))\n\n        return x"},{"cell_type":"code","execution_count":null,"id":"483a5ba6","metadata":{"jupyter":{"source_hidden":false}},"source":"class DIMSSNet(nn.Module):\n    def __init__(self):\n        super(DIMSSNet, self).__init__()\n        self.encode_sra = SRA_Encode()\n        self.decode_sra = SRA_Decode()\n        self.encode_dra = DRA_Encode()\n        self.decode_dra = DRA_Decode()\n        self.encode_sre = SRE_Encode()\n        self.decode_sre = SRE_Decode()\n        self.encode_dre = DRE_Encode()\n        self.decode_dre = DRE_Decode()\n        self.encode_srd = SRD_Encode()\n        self.decode_srd = SRD_Decode()\n        self.encode_drd = DRD_Encode()\n        self.decode_drd = DRD_Decode()\n        self.fuse_fea = Fuse_fea()\n        # self.fuse_fea = Fuse_fea_2D()\n        \n    def forward(self, x_sra, x_dra, x_sre, x_dre, x_srd, x_drd):\n        x_sra = self.encode_sra(x_sra)\n        feas_sra = self.decode_sra(x_sra)  # (B, 32, 50, 181)\n        \n        x_dra = self.encode_dra(x_dra)\n        feas_dra = self.decode_dra(x_dra)  # (B, 32, 50, 181)\n        \n        x_sre = self.encode_sre(x_sre)\n        feas_sre = self.decode_sre(x_sre)  # (B, 32, 50, 181)\n        \n        x_dre = self.encode_dre(x_dre)\n        feas_dre = self.decode_dre(x_dre)  # (B, 32, 50, 181)\n\n        x_srd = self.encode_sre(x_srd)\n        feas_srd = self.decode_sre(x_srd)  # (B, 32, 50, 181)\n        \n        x_drd = self.encode_dre(x_drd)\n        feas_drd = self.decode_dre(x_drd)  # (B, 32, 50, 181)        \n        \n        feas = self.fuse_fea(feas_sra, feas_dra, feas_sre, feas_dre, feas_srd, feas_drd) # (B, 32, 50, 181)\n\n        return feas"},{"cell_type":"code","execution_count":null,"id":"ddb913e6","metadata":{"jupyter":{"source_hidden":false}},"source":"range_size=radar_configs['range_size']\nazimuth_size=radar_configs['azimuth_size']\nelevation_size=radar_configs['elevation_size']\n\nclass Fuse_fea(nn.Module):\n    def __init__(self):\n        super(Fuse_fea, self).__init__()\n        self.convt1 = nn.Conv2d(in_channels=192, out_channels=96,\n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.convt2 = nn.Conv2d(in_channels=96, out_channels=48,\n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.convt3 = nn.Conv2d(in_channels=48, out_channels=24,\n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        self.convt4 = nn.Conv2d(in_channels=24, out_channels=n_class,\n                                kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))        \n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, feas_sra, feas_dra, feas_sre, feas_dre, feas_srd, feas_drd):\n        feas_sre = torch.sum(feas_sre, 3, keepdim=True) # (B, 32, 50, 181) -> (B, 32, 50, 1)\n        feas_sre = feas_sre.expand(-1, -1, -1, azimuth_size)\n        \n        feas_dre = torch.sum(feas_dre, 3, keepdim=True)  # (B, 32, 50, 181) -> (B, 32, 50, 1)\n        feas_dre = feas_dre.expand(-1, -1, -1, azimuth_size)\n\n        feas_srd = torch.sum(feas_srd, 3, keepdim=True)  # (B, 32, 50, 181) -> (B, 32, 50, 1)\n        feas_srd = feas_srd.expand(-1, -1, -1, azimuth_size)\n\n        feas_drd = torch.sum(feas_drd, 3, keepdim=True)  # (B, 32, 50, 181) -> (B, 32, 50, 1)\n        feas_drd = feas_drd.expand(-1, -1, -1, azimuth_size)\n\n        # Resize using bilinear interpolation\n        feas_sra = F.interpolate(feas_sra, size=(range_size, azimuth_size), mode='bilinear', align_corners=False)\n        feas_dra = F.interpolate(feas_dra, size=(range_size, azimuth_size), mode='bilinear', align_corners=False)\n        feas_sre = F.interpolate(feas_sre, size=(range_size, azimuth_size), mode='bilinear', align_corners=False)\n        feas_dre = F.interpolate(feas_dre, size=(range_size, azimuth_size), mode='bilinear', align_corners=False)\n        feas_srd = F.interpolate(feas_srd, size=(range_size, azimuth_size), mode='bilinear', align_corners=False)\n        feas_drd = F.interpolate(feas_drd, size=(range_size, azimuth_size), mode='bilinear', align_corners=False)\n        \n        feas_ra = torch.cat((feas_sra, feas_dra, feas_sre, feas_dre, feas_srd, feas_drd), 1) # 6*(B, 32, 50, 181) -> (B, 192, 50, 181)\n\n        x = self.relu(self.convt1(feas_ra))  # (B, 192, 50, 181) -> (B, 96, 50, 181)\n        x = self.relu(self.convt2(x))  # (B, 96, 50, 181) -> (B, 48, 50, 181)\n        x = self.relu(self.convt3(x))  # (B, 48, 50, 181) -> (B, 24, 50, 181)        \n        x = self.sigmoid(self.convt4(x))  # (B, 24, 50, 181) -> (B, 5, 50, 181)\n        \n        return x\n    "},{"cell_type":"markdown","id":"b8d86ece","metadata":{},"source":"# Utils"},{"cell_type":"code","execution_count":null,"id":"49705a8f","metadata":{"jupyter":{"source_hidden":false}},"source":"import scipy.io\nimport numpy as np\n\n# Define the target size\nTARGET_HEIGHT = 50\nTARGET_WIDTH = 181\n\ndef load_raw_data(file_path):\n    data = scipy.io.loadmat(file_path)\n\n    # Range-Azimuth Static\n    range_az_static = data['range_az_static']\n    range_az_static_prob = data['range_az_music_average_static_probability']\n    range_az_static_class2_prob = range_az_static_prob[:, :, 1]  # 椅子的概率\n\n    # Range-Elevation Static\n    range_ele_static = data['range_ele_static_all']\n\n    # Range-Azimuth Dynamic\n    range_az_dynamic = data['range_az_dynamic']\n    range_az_dynamic_prob = data['range_az_music_average_dynamic_probability']\n    range_az_dynamic_class1_prob = range_az_dynamic_prob[:, :, 0]  # 人的概率\n\n    # Range-Elevation Dynamic\n    range_ele_dynamic = data['range_ele_dynamic_all'] \n\n    return [range_az_static, range_az_dynamic, range_ele_static,\n            range_ele_dynamic, range_az_dynamic_class1_prob, range_az_static_class2_prob]\n\ndef generate_parameter():\n    parameter = {\n        'c': 3e8,  # the speed of light\n        'start_freq': 77e9,  # starting frequency\n        'tr': 60e-6,  # Sweeping time, that is, the cycle\n        'samples': 256,  # sampling point\n        'fs': 10e6,  # Sampling rate\n        'tframe_set': 80e-3,  # Frame period\n        'range_bin': 256,  # range bin\n        'chirps': 128,  # chirp number\n        'doppler_bin': 128,  # doppler bin\n        'slope': 29.982e12,  # chirp slope\n        'bandwidth': 29.982e12 * 60e-6,  # Effective bandwidth of the transmitted signal\n        'bandwidth_valid': 256 / 10e6 * 29.982e12,  # Transmission signal bandwidth\n        'center_freq': 77e9 + (29.982e12 * 60e-6) / 2,  # center frequency\n        'lambda': 3e8 / (77e9 + (29.982e12 * 60e-6) / 2),  # wavelength\n        'tx_antenna': [1, 1, 1],  # The number of transmitting antennas\n        'rx_antenna': [1, 1, 1, 1],  # Number of receiving antennas\n        'tx_num': 3,  # The number of transmitting antennas\n        'rx_num': 4,  # The number of receiving antennas\n        'virtual_antenna': 12,  # Number of virtual antennas\n        'dz': (3e8 / (77e9 + (29.982e12 * 60e-6) / 2)) / 2,  # Pitch spacing of the receiving antenna\n        'dx': (3e8 / (77e9 + (29.982e12 * 60e-6) / 2)) / 2,  # Horizontal spacing of receiving antennas\n        'num_cpi': 50,  # 帧数\n    }\n    return parameter\n\n"},{"cell_type":"markdown","id":"c266f24b","metadata":{},"source":"# Dataloaders"},{"cell_type":"code","execution_count":null,"id":"892cfb3a","metadata":{},"source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nclass CustomDataset(Dataset):\n    def __init__(self, file_paths, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n        self.file_names = [os.path.basename(path) for path in file_paths]\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        data = torch.load(self.file_paths[idx], weights_only=True)\n        \n        images = data[:6]  #first 6 channels are the heatmaps\n        labels = data[6]   #last channel is the label\n        \n        images = images.float()  \n        labels = labels.long()   \n        labels = labels + 1     #labels are -1 to 3, add 1 to make them 0 to 4\n\n        if self.transform:\n            images = self.transform(images)\n            labels = self.transform(labels)\n        \n        return images, labels, self.file_names[idx]\n\nclass CustomDataset_test(Dataset):\n    def __init__(self, file_paths, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n        self.file_names = [os.path.basename(path) for path in file_paths]\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    def __getitem__(self, idx):\n        data = torch.load(self.file_paths[idx], weights_only=True)\n        \n        images = data[:6] #only load first 6 channels, labels are not in val_set and test_set\n        \n        images = images.float()  \n\n        if self.transform:\n            images = self.transform(images)\n        \n        return images, self.file_names[idx]\n\n# extend base_path to file_path\ndef generate_file_paths(base_path):\n    file_paths = []\n    for frame in os.listdir(base_path):\n        frame_path = os.path.join(base_path, frame)\n        if frame_path.endswith('.mat.pt'):\n            file_paths.append(frame_path)\n    return [path for path in file_paths if os.path.exists(path)]\n\ndef load_data(base_path, batch_size=4, num_workers=2, test_size=0.2):\n    file_paths = generate_file_paths(base_path)\n    \n    train_paths, test_paths = train_test_split(file_paths, test_size=test_size, random_state=42)\n    \n    train_dataset = CustomDataset(file_paths=train_paths)\n    test_dataset = CustomDataset(file_paths=test_paths)\n    \n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=num_workers, \n        drop_last=True\n    )\n    \n    test_loader = DataLoader(\n        test_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=num_workers, \n        drop_last=True\n    )\n    \n    return train_loader, test_loader\n"},{"cell_type":"markdown","id":"67cd6c15","metadata":{},"source":"# Loss"},{"cell_type":"code","execution_count":null,"id":"4c573803","metadata":{},"source":"#Used to handle the problem of class imbalance\n\ndef to_one_hot(tensor, nClasses):\n    n, h, w = tensor.size()\n    one_hot = torch.zeros(n, nClasses, h, w, device=tensor.device)\n    index_tensor = tensor.long().view(n, 1, h, w)\n    one_hot = one_hot.scatter_(1, index_tensor, 1)\n    return one_hot\n\ndef _neg_loss(pred, gt):\n    pred = torch.clamp(pred, 1e-5, 1-1e-5)\n\n    pos_inds = gt.eq(1).float()\n    neg_inds = gt.eq(0).float()\n\n    balance_cof = 1.0\n    pos_loss = torch.log(pred) * torch.pow(1 - pred, 2) * pos_inds * balance_cof\n    neg_loss = torch.log(1 - pred) * torch.pow(pred, 2) * neg_inds\n    num_pos = pos_inds.float().sum()\n    pos_loss_sum = pos_loss.sum()\n    neg_loss_sum = neg_loss.sum()\n    if num_pos == 0:\n        loss = -neg_loss_sum\n    else:\n        loss = -(pos_loss_sum + neg_loss_sum) / num_pos\n    return loss\n\nclass FocalLoss(nn.Module):\n    def __init__(self):\n        super(FocalLoss, self).__init__()\n        self.neg_loss = _neg_loss\n\n    def forward(self, out, target):\n        return self.neg_loss(out, target)"},{"cell_type":"markdown","id":"dec2b582","metadata":{},"source":"# Train"},{"cell_type":"code","execution_count":null,"id":"7c2f0301","metadata":{},"source":"import torch.optim as optim\nimport matplotlib.pyplot as plt\nimport time\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nnum_epochs = 20\n\nbase_path = '/bohr/train-22pn/v2/training_set'\ntrain_loader, test_loader = load_data(\n    base_path=base_path,\n    batch_size=4,\n    num_workers=2,\n    test_size=0.2\n)\n\ndimssnet = DIMSSNet().to(device)\ncriterion = FocalLoss()\noptimizer = optim.Adam(dimssnet.parameters(), lr=0.001)\nstart_time = time.time()\ntest_loss_record = []\ntrain_loss_record = []\n\nfor epoch in range(num_epochs):\n    dimssnet.train()\n    running_loss = 0.0\n    for images, labels, _ in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        images = [image.unsqueeze(1) for image in images]\n        images = torch.stack(images, dim=0)\n        images = images.float()\n        \n        outputs = dimssnet(images[:, 0, :, :], images[:, 1, :, :], images[:, 2, :, :],\n                            images[:, 3, :, :], images[:, 4, :, :], images[:, 5, :, :])\n\n        outputs_resized = F.interpolate(outputs, size=(TARGET_HEIGHT, TARGET_WIDTH), mode='bilinear', align_corners=False)\n        labels = to_one_hot(labels, n_class)\n        \n        loss = criterion(outputs_resized, labels.long())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n    test_loss = 0.0\n    dimssnet.eval()\n    with torch.no_grad():\n        for images, labels, _ in test_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            images = [image.unsqueeze(1) for image in images]\n            images = torch.stack(images, dim=0)\n            images = images.float()\n            \n            outputs = dimssnet(images[:, 0, :, :], images[:, 1, :, :], images[:, 2, :, :],\n                                images[:, 3, :, :], images[:, 4, :, :], images[:, 5, :, :])\n            \n            outputs_resized = F.interpolate(outputs, size=(TARGET_HEIGHT, TARGET_WIDTH), mode='bilinear', align_corners=False)\n            labels = to_one_hot(labels, n_class)\n            \n            loss = criterion(outputs_resized, labels.long())\n            test_loss += loss.item()\n    \n    test_loss_record.append(test_loss/len(test_loader))\n    train_loss_record.append(running_loss/len(train_loader))\n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {running_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}\")\n\n# Create a new state_dict and add the \"model\" prefix, in order to match the loading method in metrics\nnew_state_dict = {}\nfor k, v in dimssnet.state_dict().items():\n    new_key = f\"model.{k}\" \n    new_state_dict[new_key] = v\n\nprint(\"dimssnet successfully saved!\")    \nprint(\"Training finished.\")\nend_time = time.time()\ntotal_time = end_time - start_time\nprint(f\"Training completed in {total_time} seconds\")"},{"cell_type":"markdown","id":"f001aee8","metadata":{},"source":"# Inference"},{"cell_type":"code","execution_count":null,"id":"8b330819","metadata":{},"source":"# Run inference on validation set and testing set\nimport pandas as pd\n\ndef run_inference(model, data_loader):\n    model.eval()\n    predictions = []\n    filenames = []\n    \n    with torch.no_grad():\n        for images, file_names in data_loader:\n            images = images.cuda() if torch.cuda.is_available() else images\n            \n            x = [image.unsqueeze(1) for image in images]\n            x = torch.stack(x, dim=0)\n            x = x.float()\n            \n            # Forward propagation\n            outputs = model(x[:, 0, :, :], x[:, 1, :, :], x[:, 2, :, :],\n                            x[:, 3, :, :], x[:, 4, :, :], x[:, 5, :, :])\n            \n            # Adjust the output size\n            outputs = F.interpolate(outputs, size=(50, 181), mode='bilinear', align_corners=False)\n\n            preds = torch.argmax(outputs, dim=1)\n            \n            preds = preds - 1\n            \n            for i, pred in enumerate(preds):\n                predictions.append(pred.cpu().numpy().flatten())\n                filenames.append(file_names[i])\n    \n    return predictions, filenames\n\n# Load validation set\nif os.environ.get('DATA_PATH'):\n    DATA_PATH = os.environ.get(\"DATA_PATH\") + \"/\" \nelse:\n    DATA_PATH = \"\"  # Fallback for local testing\n# Load validation set\nval_paths = generate_file_paths(DATA_PATH + 'validation_set')\nval_dataset = CustomDataset_test(file_paths=val_paths)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=0\n)\n\n# Load testing set\nif os.environ.get('DATA_PATH'):\n    DATA_PATH = os.environ.get(\"DATA_PATH\") + \"/\" \nelse:\n    DATA_PATH = \"\"  # Fallback for local testing\n# Load testing set\ntest_paths = generate_file_paths(DATA_PATH + 'testing_set')\ntest_dataset = CustomDataset_test(file_paths=test_paths)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=1,\n    shuffle=False,\n    num_workers=0\n)\n\n# Run inference on validation set\nprint(\"Running inference on validation set...\")\nval_predictions, val_filenames = run_inference(dimssnet, val_loader)\n\n# Save validation results to CSV\nval_results = []\nfor filename, pred in zip(val_filenames, val_predictions):\n    # Create a row with filename and flattened predictions\n    row = {'filename': filename}\n    for i, p in enumerate(pred):\n        row[f'pixel_{i}'] = p\n    val_results.append(row)\n\nval_df = pd.DataFrame(val_results)\nval_df.to_csv('submission_val.csv', index=False)\nprint(f\"Validation results saved to output_validation.csv with shape: {val_df.shape}\")\n\n# Run inference on testing set\nprint(\"Running inference on testing set...\")\ntest_predictions, test_filenames = run_inference(dimssnet, test_loader)\n\n# Save testing results to CSV\ntest_results = []\nfor filename, pred in zip(test_filenames, test_predictions):\n    # Create a row with filename and flattened predictions\n    row = {'filename': filename}\n    for i, p in enumerate(pred):\n        row[f'pixel_{i}'] = p\n    test_results.append(row)\n\ntest_df = pd.DataFrame(test_results)\ntest_df.to_csv('submission_test.csv', index=False)\nprint(f\"Testing results saved to output_testing.csv with shape: {test_df.shape}\")\n\nprint(\"\\nInference completed! Results saved to:\")\nprint(\"- submission_val_ref.csv (for validation set leaderboard)\")\nprint(\"- submission_test_ref.csv (for testing set leaderboard)\")"},{"cell_type":"code","execution_count":null,"id":"72fde72d","metadata":{},"source":"import zipfile\nimport os\n\n# Define the files to zip and the zip file name.\nfiles_to_zip = ['submission_val.csv', 'submission_test.csv']\nzip_filename = 'submission.zip'\n\n# Create a zip file\nwith zipfile.ZipFile(zip_filename, 'w') as zipf:\n    for file in files_to_zip:\n        # Add the file to the zip fil\n        zipf.write(file, os.path.basename(file))\n\nprint(f'{zip_filename} Created successfully!')"}],"metadata":{"kernelspec":{"display_name":"ioai","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.8"}},"nbformat":4,"nbformat_minor":5}